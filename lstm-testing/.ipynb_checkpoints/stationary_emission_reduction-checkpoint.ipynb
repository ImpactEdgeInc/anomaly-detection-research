{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ee14b5c4-5839-4d3c-a9e6-39d1ba1d3bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, make_scorer\n",
    "from sklearn.utils.multiclass import type_of_target\n",
    "\n",
    "import psycopg2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "import os  # <-- NEW: Import the os module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "105f714a-4393-40b9-9d8d-6c6bafa0b3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv(dotenv_path='database.env')\n",
    "\n",
    "# --- Configuration ---\n",
    "DB_NAME = os.environ['DB_NAME']\n",
    "DB_USER = os.environ['DB_USER']\n",
    "DB_PASS = os.environ['DB_PASS']\n",
    "HOST = os.environ['HOST']\n",
    "PORT = os.environ['PORT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "f43e50db-a15a-47a2-b9b0-11d4809cb767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                facilityId          co2e  \\\n",
      "0     68f4851d-4959-49b6-96a1-63d80c816ed3      0.000000   \n",
      "1     bf2278d8-0d33-4e28-a55e-ad3c1537af7e      0.000054   \n",
      "2     4f4bf05a-0a52-48db-b66d-3b3da6b23619      0.000054   \n",
      "3     4f4bf05a-0a52-48db-b66d-3b3da6b23619      0.004033   \n",
      "4     4f4bf05a-0a52-48db-b66d-3b3da6b23619      0.010790   \n",
      "...                                    ...           ...   \n",
      "6160  beae4852-c882-4706-8dd9-f8ce4dc11989      0.222116   \n",
      "6161  d0f9256f-17f4-4b7e-abc7-2eeeba8d59c0      0.053958   \n",
      "6162  d0f9256f-17f4-4b7e-abc7-2eeeba8d59c0   9049.547300   \n",
      "6163  e0bd2a9c-69d4-403e-beed-3b9a6b08b382  12163.778891   \n",
      "6164  e0bd2a9c-69d4-403e-beed-3b9a6b08b382   6588.125943   \n",
      "\n",
      "                           fuelHint                         createdAt  \\\n",
      "0                            Bamboo  2025-01-05 21:53:26.836000-08:00   \n",
      "1                       Natural Gas  2025-01-20 00:33:24.881000-08:00   \n",
      "2                       Natural Gas  2025-01-20 02:59:01.888000-08:00   \n",
      "3                       Natural Gas  2025-01-20 02:59:02.308000-08:00   \n",
      "4                       Natural Gas  2025-01-20 02:59:02.737000-08:00   \n",
      "...                             ...                               ...   \n",
      "6160                      Rice Husk  2025-07-15 01:38:17.376000-07:00   \n",
      "6161                            HSD  2025-09-01 23:40:19.098000-07:00   \n",
      "6162                           Coal  2025-09-01 23:41:02.578000-07:00   \n",
      "6163  Natural Gas (Reformer Burner)  2025-09-22 23:49:02.407000-07:00   \n",
      "6164   Natural Gas (Boiler+HRSG+GT)  2025-09-22 23:50:08.849000-07:00   \n",
      "\n",
      "           ch4       n2o           ef           co2  \n",
      "0     0.000000  0.000000    93.864500      0.000000  \n",
      "1     0.000001  0.000000    53.114500      0.000054  \n",
      "2     0.000001  0.000000    53.114500      0.000054  \n",
      "3     0.000076  0.000008    53.114500      0.004029  \n",
      "4     0.000203  0.000020    53.114500      0.010779  \n",
      "...        ...       ...          ...           ...  \n",
      "6160  0.214951  0.013614     0.031000      0.000000  \n",
      "6161  0.000003  0.000001  2697.910000      0.053635  \n",
      "6162  0.537400  0.161220  1683.950000   9002.524800  \n",
      "6163  0.482067  0.024103     2.170000  12051.670330  \n",
      "6164  0.261096  0.013055     2.170000   6527.405889  \n",
      "\n",
      "[6165 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "# 2. Database Connection and Setup\n",
    "conn = psycopg2.connect(\n",
    "    dbname=DB_NAME, user=DB_USER, password=DB_PASS, host=HOST, port=PORT\n",
    ")\n",
    "conn.autocommit = True\n",
    "cur = conn.cursor()\n",
    "\n",
    "cur.execute('SELECT \"facilityId\",\"co2e\",\"fuelHint\",\"createdAt\",\"ch4\",\"n2o\",\"ef\",\"co2\" FROM stationary_combustion_activity')\n",
    "\n",
    "data = pd.DataFrame(cur.fetchall(), columns = [\"facilityId\",\"co2e\",\"fuelHint\",\"createdAt\",\"ch4\",\"n2o\",\"ef\",\"co2\"])\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "8ff3c8fc-2ca5-42ad-9504-9ba3cdfdb4de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_fac_train shape: (4928,)\n",
      "X_other_train shape: (4928, 5, 35)\n",
      "y_train shape: (4928,)\n"
     ]
    }
   ],
   "source": [
    "# 1. Convert 'createdAt' to datetime objects and sort the data\n",
    "data['createdAt'] = pd.to_datetime(data['createdAt'], utc=True)\n",
    "data = data.sort_values(by='createdAt').reset_index(drop=True)\n",
    "\n",
    "# 2. Engineer Temporal Feature (as Unix timestamp)\n",
    "data['timestamp'] = data['createdAt'].apply(lambda x: x.timestamp())\n",
    "\n",
    "# --- 3. Embeddable Categorical Feature: facilityId ---\n",
    "le_facility = LabelEncoder()\n",
    "data['facilityId_encoded'] = le_facility.fit_transform(data['facilityId'])\n",
    "VOCAB_SIZE_FACILITY = len(le_facility.classes_)\n",
    "# Heuristic for Embedding Dimension\n",
    "EMBEDDING_DIM_FACILITY = min(50, int(np.ceil(VOCAB_SIZE_FACILITY / 2)))\n",
    "\n",
    "# --- 4. Other Features: One-Hot Encode fuelHint and Drop Original Strings ---\n",
    "# First, create one-hot columns for 'fuelHint'\n",
    "data_encoded = pd.get_dummies(data, columns=['fuelHint'], dtype=int)\n",
    "\n",
    "# Now, drop ALL non-numerical columns that are not the target or a desired numerical feature\n",
    "# We drop the original string columns: 'facilityId', 'createdAt', 'co2e', and 'facilityId' string\n",
    "data_encoded = data_encoded.drop(columns=['facilityId', 'createdAt'])\n",
    "\n",
    "# 5. Prepare Target (Y) and scale it\n",
    "TARGET_VAR = \"co2e\"\n",
    "\n",
    "# *** CRITICAL FIX: Convert column to float BEFORE calculating quantile ***\n",
    "data_encoded[TARGET_VAR] = data_encoded[TARGET_VAR].astype(float)\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "data_encoded['ch4'] = data_encoded['ch4'].astype(float)\n",
    "data_encoded['n2o'] = data_encoded['n2o'].astype(float)\n",
    "data_encoded['ef'] = data_encoded['ef'].astype(float)\n",
    "data_encoded['co2'] = data_encoded['co2'].astype(float)\n",
    "\n",
    "# Cap at the 99.9th percentile to mitigate extreme outliers before log transformation\n",
    "quantile_999 = data_encoded[TARGET_VAR].quantile(0.999)\n",
    "data_encoded[TARGET_VAR] = data_encoded[TARGET_VAR].clip(upper=quantile_999)\n",
    "# --------------------------------------------------\n",
    "\n",
    "Y = data_encoded[TARGET_VAR].values.astype('float32')\n",
    "Y_log = np.log1p(Y) \n",
    "Y_log = Y_log.reshape(-1, 1) \n",
    "scaler_Y = StandardScaler()\n",
    "Y_scaled = scaler_Y.fit_transform(Y_log) \n",
    "\n",
    "# --- 6. Define two separate feature inputs for the Multi-Input Model ---\n",
    "\n",
    "# Input 1: The label-encoded facility ID at time t+5 (for the Embedding Layer)\n",
    "# Note: We keep this as an integer array, NOT scaled.\n",
    "X_facility = data_encoded['facilityId_encoded'].values.astype('float32') \n",
    "\n",
    "# Input 2: All other numerical features (sequence of 5 steps for the LSTM)\n",
    "# Add ch4, n2o, and ef to the other features input\n",
    "ADDITIONAL_NUMERIC_COLS = ['ch4', 'n2o', 'co2', 'ef', 'timestamp']\n",
    "FUEL_HINT_COLS = [col for col in data_encoded.columns if col.startswith('fuelHint_')]\n",
    "\n",
    "# Combine all \"other\" features\n",
    "OTHER_FEATURES_COLS = ADDITIONAL_NUMERIC_COLS + FUEL_HINT_COLS\n",
    "\n",
    "X_other = data_encoded[OTHER_FEATURES_COLS].values.astype('float32')\n",
    "\n",
    "# 7. Define time step and sequence creation function\n",
    "time_step = 5\n",
    "\n",
    "def create_sequences_multiple_inputs(X_facility, X_other, Y, time_steps):\n",
    "    X_fac_seq, X_other_seq, y_seq = [], [], []\n",
    "    # We stop 'time_steps' short of the end to create the sequences\n",
    "    for i in range(len(X_other) - time_steps):\n",
    "        # Input 1: The single facility ID at time t+time_steps\n",
    "        X_fac_seq.append(X_facility[i + time_steps]) \n",
    "        # Input 2: Sequence of other features (time_steps x N_features)\n",
    "        X_other_seq.append(X_other[i:(i + time_steps), :]) \n",
    "        # Target: co2e at time t+time_steps\n",
    "        y_seq.append(Y[i + time_steps, 0])\n",
    "        \n",
    "    return np.array(X_fac_seq), np.array(X_other_seq), np.array(y_seq)\n",
    "\n",
    "# 8. Scale the 'other' features (Input 2)\n",
    "scaler_X_other = MinMaxScaler(feature_range=(0, 1))\n",
    "X_other_scaled = scaler_X_other.fit_transform(X_other)\n",
    "\n",
    "# 9. Create the new sequences\n",
    "X_fac, X_other_seq, y1 = create_sequences_multiple_inputs(\n",
    "    X_facility, X_other_scaled, Y_scaled, time_step\n",
    ")\n",
    "\n",
    "# 10. Split the data chronologically (correct approach for time series)\n",
    "split_point = int(len(y1) * 0.8)\n",
    "X_fac_train, X_fac_test = X_fac[:split_point], X_fac[split_point:]\n",
    "X_other_train, X_other_test = X_other_seq[:split_point], X_other_seq[split_point:]\n",
    "y_train, y_test = y1[:split_point], y1[split_point:]\n",
    "\n",
    "print(f\"X_fac_train shape: {X_fac_train.shape}\")\n",
    "print(f\"X_other_train shape: {X_other_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "07600e81-968e-4dba-8ff0-4f82c9ef9a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Concatenate, LSTM, Dense, Dropout, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Ensure VOCAB_SIZE_FACILITY, EMBEDDING_DIM_FACILITY, time_step, and X_other_train \n",
    "# are available from Cell 9.\n",
    "\n",
    "def create_embedded_lstm_model(dropout_rate=0.4, lstm_units=64):\n",
    "    \n",
    "    # --- Input 1: Facility ID (Single integer at time t+5) ---\n",
    "    input_fac = Input(shape=(1,), name='facility_input')\n",
    "    \n",
    "    # Embedding layer: Converts the integer ID to a dense vector\n",
    "    embedding = Embedding(\n",
    "        input_dim=VOCAB_SIZE_FACILITY, \n",
    "        output_dim=EMBEDDING_DIM_FACILITY,\n",
    "        # input_length=1 is deprecated, but keep the argument structure\n",
    "        input_length=1\n",
    "    )(input_fac)\n",
    "    \n",
    "    # Flatten the 3D output (batch, 1, dim) to 2D (batch, dim) for concatenation\n",
    "    facility_features = Flatten()(embedding)\n",
    "\n",
    "    # --- Input 2: Other Features (Sequence of 5 time steps) ---\n",
    "    # N_OTHER_FEATURES must be derived from your prepared data shape\n",
    "    N_OTHER_FEATURES = X_other_train.shape[2] \n",
    "    input_other = Input(shape=(time_step, N_OTHER_FEATURES), name='other_features_input')\n",
    "    \n",
    "    # Stacked LSTM for the time-series data\n",
    "    lstm_output = LSTM(units=lstm_units, return_sequences=True, activation='relu')(input_other)\n",
    "    lstm_output = Dropout(dropout_rate)(lstm_output)\n",
    "    lstm_output = LSTM(units=lstm_units // 2, activation='relu')(lstm_output)\n",
    "    lstm_output = Dropout(dropout_rate)(lstm_output)\n",
    "\n",
    "    # --- Merge the two paths ---\n",
    "    merged = Concatenate()([lstm_output, facility_features])\n",
    "    \n",
    "    # --- Final Prediction Layer ---\n",
    "    output = Dense(1)(merged) # No activation for regression, predicting scaled (0-1) output\n",
    "    \n",
    "    # Define and compile the final model\n",
    "    model = Model(inputs=[input_fac, input_other], outputs=output)\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "    \n",
    "    # *** CRITICAL CHANGE: Using MAE loss to handle outliers ***\n",
    "    model.compile(loss='mean_absolute_error', optimizer=optimizer)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "0e09151c-36ac-496b-bfa5-f52965afafd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aakash\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m154/154\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 16ms/step - loss: 0.0919 - val_loss: 1.4077\n",
      "Epoch 2/100\n",
      "\u001b[1m154/154\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 0.0655 - val_loss: 1.5127\n",
      "Epoch 3/100\n",
      "\u001b[1m154/154\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0480 - val_loss: 1.4192\n",
      "Epoch 4/100\n",
      "\u001b[1m154/154\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0289 - val_loss: 1.4338\n",
      "Epoch 5/100\n",
      "\u001b[1m154/154\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0225 - val_loss: 1.4065\n",
      "Epoch 6/100\n",
      "\u001b[1m154/154\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0221 - val_loss: 1.4303\n",
      "Epoch 7/100\n",
      "\u001b[1m154/154\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0159 - val_loss: 1.4361\n",
      "Epoch 8/100\n",
      "\u001b[1m154/154\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0166 - val_loss: 1.4467\n",
      "Epoch 9/100\n",
      "\u001b[1m154/154\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0130 - val_loss: 1.4304\n",
      "Epoch 10/100\n",
      "\u001b[1m154/154\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0101 - val_loss: 1.4353\n",
      "Epoch 11/100\n",
      "\u001b[1m154/154\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 0.0112 - val_loss: 1.4245\n",
      "Epoch 12/100\n",
      "\u001b[1m154/154\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0111 - val_loss: 1.4328\n",
      "Epoch 13/100\n",
      "\u001b[1m154/154\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0086 - val_loss: 1.4319\n",
      "Epoch 14/100\n",
      "\u001b[1m154/154\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0095 - val_loss: 1.4348\n",
      "Epoch 15/100\n",
      "\u001b[1m154/154\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0067 - val_loss: 1.4342\n",
      "Epoch 15: early stopping\n",
      "Restoring model weights from the end of the best epoch: 5.\n",
      "\u001b[1m39/39\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step\n",
      "----------------------------------------\n",
      "âœ¨ Model Performance on Actual CO2e Units âœ¨\n",
      "R2 Score:                 -0.1159\n",
      "Mean Squared Error (MSE): 37,448,044.00\n",
      "Mean Absolute Error (MAE): 1,972.33\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "# Re-define constants or ensure they are available from Cell 9/57\n",
    "BEST_LSTM_UNITS = 64\n",
    "BEST_DROPOUT_RATE = 0.4\n",
    "BEST_BATCH_SIZE = 32\n",
    "BEST_EPOCHS = 100 # Set high, EarlyStopping will cut it short\n",
    "\n",
    "# 1. Create the model\n",
    "best_model_keras = create_embedded_lstm_model(\n",
    "    dropout_rate=BEST_DROPOUT_RATE, \n",
    "    lstm_units=BEST_LSTM_UNITS\n",
    ")\n",
    "\n",
    "# 2. Define the callback\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    patience=10, # Wait 10 epochs for improvement\n",
    "    restore_best_weights=True, # Use weights from the best epoch\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 3. Train the model\n",
    "history = best_model_keras.fit(\n",
    "    x=[X_fac_train, X_other_train], # Correct format for Keras Functional API\n",
    "    y=y_train,\n",
    "    batch_size=BEST_BATCH_SIZE,\n",
    "    epochs=BEST_EPOCHS,\n",
    "    validation_data=([X_fac_test, X_other_test], y_test),\n",
    "    callbacks=[early_stop], # *** ADDED EARLY STOPPING ***\n",
    "    verbose=1,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# 1. Generate predictions on the scaled test data\n",
    "y_pred_scaled = best_model_keras.predict([X_fac_test, X_other_test])\n",
    "\n",
    "# 2. Inverse transform from (0, 1) scale to log-scale\n",
    "# Note: y_test and y_pred_scaled are currently (N_samples, 1).\n",
    "# We must use the scaler_Y object defined in Cell 9.\n",
    "y_test_log = scaler_Y.inverse_transform(y_test.reshape(-1, 1))\n",
    "y_pred_log = scaler_Y.inverse_transform(y_pred_scaled)\n",
    "\n",
    "# 3. Inverse transform from log-scale to original CO2e units (using np.expm1)\n",
    "# np.expm1(x) is the inverse of np.log1p(x) (which is what you used: log(1+x))\n",
    "y_test_actual = np.expm1(y_test_log)\n",
    "y_pred_actual = np.expm1(y_pred_log)\n",
    "\n",
    "# R2 Score\n",
    "final_r2_score = r2_score(y_test_actual, y_pred_actual)\n",
    "\n",
    "# Mean Squared Error (MSE)\n",
    "final_mse = mean_squared_error(y_test_actual, y_pred_actual)\n",
    "\n",
    "# Mean Absolute Error (MAE)\n",
    "final_mae = mean_absolute_error(y_test_actual, y_pred_actual)\n",
    "\n",
    "print(\"-\" * 40)\n",
    "print(\"âœ¨ Model Performance on Actual CO2e Units âœ¨\")\n",
    "print(f\"R2 Score:                 {final_r2_score:.4f}\")\n",
    "print(f\"Mean Squared Error (MSE): {final_mse:,.2f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {final_mae:,.2f}\")\n",
    "print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "9cf91da3-5156-4a54-baa5-0a1b369f0313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------\n",
      "ðŸ” Outlier Diagnostics (Actual CO2e Units)\n",
      "Max True CO2e Value (Test Set): 51,630.03\n",
      "Max Predicted CO2e Value:       0.18\n",
      "Max Absolute Prediction Error:  51,629.91\n",
      "-------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# --- RERUN DIAGNOSTICS FROM PREVIOUS STEP ---\n",
    "# Assuming y_test_actual and y_pred_actual are available from the previous run\n",
    "absolute_errors = np.abs(y_test_actual - y_pred_actual)\n",
    "\n",
    "print(\"-\" * 55)\n",
    "print(\"ðŸ” Outlier Diagnostics (Actual CO2e Units)\")\n",
    "print(f\"Max True CO2e Value (Test Set): {y_test_actual.max():,.2f}\")\n",
    "print(f\"Max Predicted CO2e Value:       {y_pred_actual.max():,.2f}\")\n",
    "print(f\"Max Absolute Prediction Error:  {absolute_errors.max():,.2f}\")\n",
    "print(\"-\" * 55)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c1f5d0-01a3-444f-b9ee-18af92eed0e5",
   "metadata": {},
   "source": [
    "It looks like even with more advanced regularization and more predictor variables, our LSTM model is unable to predict stationary combustion given the variables that it has. The R<sup>2</sup> score stays consistent regardless of the variables used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "501a4e49-6dc0-454f-8cea-f4df9ee13ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "cur.close()\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
